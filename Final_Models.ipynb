{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "infectious-banner",
   "metadata": {},
   "source": [
    "# Final Project\n",
    "by Kris Johnson"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "chinese-cabinet",
   "metadata": {},
   "source": [
    "# Research Goal\n",
    "---\n",
    "\n",
    "Can we accurately classify Bitcoin transactions into either Ransomware or non-ransomware categories?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "planned-mercy",
   "metadata": {},
   "source": [
    "\n",
    "Dataset from [UCI Bitcoin Heist Ransomware](https://archive.ics.uci.edu/ml/datasets/BitcoinHeistRansomwareAddressDataset#). Brief Description from UCI: \n",
    "\n",
    "    Entire Bitcoin transaction graph from 2009 January to 2018 December. Using a time interval of 24 hours, daily transactions on the network to form the Bitcoin graph. Filtered out network edges that transfer less than B0.3, as ransom amounts are rarely below this threshold.\n",
    "\n",
    "Since this is equivalent to searching for rare fraud events, it's of more use to catch the fraud with less concern for flagging normal transactions as fraudulent. Even more so since some of the labeled normal data actually could be fraudulent: \"Note that although we are certain about ransomware labels, we do not know if all white addresses are in fact not related to ransomware.\"\n",
    "\n",
    "Issues:\n",
    "- Massive Class imbalance\n",
    "    - original data was 1.4% ransomWare vs 98.6% normal\n",
    "- Don't care which type of ransomware\n",
    "    - predict Binary Fraud or not\n",
    "    - could classify type of ransomWare attack, but not useful from a business case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "small-helmet",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports - collab has issues with importing only imblearn\n",
    "import warnings # for dealing with non-convergence for default logistic regression\n",
    "import imblearn\n",
    "from   imblearn                   import under_sampling, over_sampling\n",
    "from   imblearn.pipeline          import make_pipeline # scikit-learn Pipeline does not work with imblearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from   sklearn.ensemble           import RandomForestClassifier\n",
    "from   sklearn.linear_model       import LogisticRegression\n",
    "from   sklearn.preprocessing      import StandardScaler\n",
    "from   sklearn.decomposition      import PCA\n",
    "from   sklearn.metrics            import balanced_accuracy_score, recall_score, precision_score\n",
    "from   sklearn.metrics            import make_scorer, fbeta_score\n",
    "\n",
    "from   sklearn.model_selection    import RandomizedSearchCV\n",
    "from   sklearn.model_selection    import cross_val_score\n",
    "\n",
    "from   sklearn.model_selection    import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "skilled-reading",
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seed = 42  # set to None to turn off seeding. Using to replicate results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "architectural-syntax",
   "metadata": {},
   "source": [
    "# Load Preprocessed Data, Split into Train and Test Sets\n",
    "Separately performed EDA and created possibly predictive extra columns using cumulative sums, account (wallet address) age, and time since last activity\n",
    "\n",
    "The data has been downsampled from the original 2.9 million records to 558,964 records to satisfy size requirements. Only the majority class (non-Ransomware) was downsampled to preserve the precious few minority labels.\n",
    "\n",
    "41,413 RansomWare labels to 517,551 normal transaction labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "rocky-static",
   "metadata": {},
   "outputs": [],
   "source": [
    "remote_location = 'https://raw.githubusercontent.com/krisrjohnson/ml_lab_final_data/master/Bitcoin_Heist_min_df.csv'\n",
    "df = pd.read_csv(remote_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "medical-broadcasting",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[df.columns[3:-1]]  # dropping 'address','year', and 'day'\n",
    "X = X.drop('Date', axis=1)  # remove target variable\n",
    "X = X.drop('label', axis=1)  # remove target variable\n",
    "\n",
    "y = df['label']\n",
    "y = np.ravel(y)\n",
    "y = np.not_equal(y, 'white').astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "sealed-austria",
   "metadata": {},
   "outputs": [],
   "source": [
    "# hold out a final test set, using random_state so results are reproducible\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y, stratify=y, random_state=random_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "geographic-expense",
   "metadata": {},
   "source": [
    "# Randomized Cross Validation Search to get best params\n",
    "\n",
    "\n",
    "Becuase we're trying to predict RansomWare, a very rare event, we want a very high Recall. However, fitting to recall only would lead to predicting everything as RansomWare, since that'd be equivalent to a recall of 1. Recall measures how many of the ransomWare's in the data we found, with no downside to guessing wrong. The actual downside to guessing wrong can be captured with Precision, which measures how many of our ransomWare predictions were correct. And the f-score is a way to average Precision and Recall, using the Harmonic mean. So to fully contextualize our results we'll really need all three metrics. For the f-score, the basic Harmonic mean is typically referred to as F1 score. We'll weight it to lean to favoring higher recall at the cost of precision by using a Beta parameter, in this case 2, typically referred to as the F2 score.\n",
    "\n",
    "Code for making your own scorer directly from the [docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.make_scorer.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "loving-protection",
   "metadata": {},
   "source": [
    "### Pipes and Defaults case\n",
    "\n",
    "Pipes are very useful to manage a lot of the machine learning overhead. Here I create pipes to enable Cross Validation with randomized parameter searching. \n",
    "\n",
    "The Logistic Regression pipe use the SMOTE technique first to create synthetic data from the minority class as a form of over-sampling the minority. We only have numeric columns, which is a requirement for SMOTE. The sampling strategy infers how many to minority labels to synthesize to get the ratio of minority to majority labels to that sampling strategy decimal argument. The k_neighbors indicates how many datapoints to use to synthesize values in between.\n",
    "\n",
    "Then for logistic I use a StandardScaler to automatically normalize and keep track of those normalizations. This insures the proper normalization gets applied to any validation or test set that get predicted from this pipeline later on, and is a main benefit of using pipes. Otherwise we'd have to manually maintain these values. PCA is used to compress help compress the data further to just its main components, or \"directions\" of most variability. Again the pipeline tracks how to convert into the main component directions, so I don't have to when doing predictions on the validation and test set. I search over the number of components to output to feed to logistic regression during cross validation. Finally I can then use logistic regression to perform binary classification, with class weighting of balanced to make sure errors on my ransomware class count for their proportionate amount. Balanced means the log loss function will heavily penalize errors involving minority classes, which are seen much rarer due to data imabalance. The 'saga' solver is better at \n",
    "\n",
    "The RandomForest pipeline implements undersampling on the majority class to get the minority to majority class ratio more aligned, based on the sampling strategy argument. Because the dataset is so large, relative to processing on a single machine, undersampling allows more training even though it's even more information loss with our original downsampling. \n",
    "\n",
    "The RandomForest classifier is setup to experiment on the number of trees to fit, where more trees means it's more likely to approach the asymptotic best fit at the cost of training time. And the max depth parameter is for determining how deep each decision tree is allowed to get, with deeper trees leading to more overfitting but shallow trees unlikely to capture the relationship between our input variables and our target.\n",
    "\n",
    "I run a cross_val_score without supplying hyperparameters to get the default scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "legitimate-constant",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe = make_pipeline(\n",
    "                         imblearn.over_sampling.SMOTE(n_jobs=-1),\n",
    "                         StandardScaler(),\n",
    "                         PCA(),\n",
    "                         LogisticRegression(solver='saga',  # better for larger datasets\n",
    "                                            class_weight='balanced',\n",
    "                                            random_state=random_seed)  # so results are replicatable\n",
    "                    )\n",
    "\n",
    "log_params = dict(smote__sampling_strategy     = np.arange(0,1,0.1),  # what new ratio of minority class to majority should be\n",
    "                  smote__k_neighbors           = [1,3,5,7,10],  # number of neighbors to use to construct synthetic data\n",
    "                  pca__n_components            = [2,3,5],  # How many main PCA \"directions\" to use\n",
    "                  logisticregression__C        = np.logspace(0, 4, 10) )  # 1/C is regularization strength, larger C means less regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "gross-israel",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_pipe = make_pipeline(\n",
    "                        imblearn.under_sampling.RandomUnderSampler(),\n",
    "                        RandomForestClassifier(n_jobs=-1, \n",
    "                                               class_weight='balanced',\n",
    "                                               random_state=random_seed)  # so results are replicatable\n",
    "                       )\n",
    "\n",
    "rf_params = dict(randomundersampler__sampling_strategy  = [.5, .75],  # new ratio of minority class to majority, .5 is even split, .75 is 3:1 minority\n",
    "                 randomforestclassifier__n_estimators   = [100, 200, 400],  # number of Trees to fit, 100 is default\n",
    "                 randomforestclassifier__max_depth      = [_**2 for _ in range(3,8)]  # max depth of fitted trees\n",
    "                )\n",
    "                 #  smote__k_neighbors                   = [4],"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "pleasant-henry",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using f-score with beta=2 to over-emphasize Recall\n",
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "supreme-beach",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall 0.49397932215010876\n",
      "Precision 0.31259532482194136\n",
      "F2 0.4421033736061286\n",
      "\n",
      "Recall 0.8609464283094449\n",
      "Precision 0.3115339487941265\n",
      "F2 0.6393018634013248\n",
      "\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "algos = [(log_pipe, None), (rf_pipe, None)]\n",
    "\n",
    "for algo, params in algos:\n",
    "    algo.fit(X_train, y_train)\n",
    "\n",
    "    recall = cross_val_score(algo, X_train, y_train, cv=3, scoring='recall')\n",
    "    print('Recall', np.mean(recall))\n",
    "    \n",
    "    precision = cross_val_score(algo, X_train, y_train, cv=3, scoring='precision')\n",
    "    print('Precision', np.mean(precision))\n",
    "    \n",
    "    f2 = cross_val_score(algo, X_train, y_train, cv=3, scoring=ftwo_scorer)\n",
    "    print('F2', np.mean(f2), end='\\n\\n')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "structured-highlight",
   "metadata": {},
   "source": [
    "So for the defaults at least RandomForest looks more promising, with a default Recall of .86, with the trade off of a really low precision score.\n",
    "\n",
    "RandomForests:\n",
    "\n",
    "    Recall 0.8632324533161622\n",
    "    Precision 0.31588642191599725\n",
    "    F2 0.6410036453072456"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "foster-score",
   "metadata": {},
   "source": [
    "## Do the RandomizedSearch \n",
    "Random seed is set in the pipelines above so results will be replicatable\n",
    "\n",
    "Fitting both pipelines using the custom F2 score described above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-vulnerability",
   "metadata": {},
   "outputs": [],
   "source": [
    "ftwo_scorer = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "played-addiction",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 20 candidates, totalling 60 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  42 tasks      | elapsed:  1.1min\n",
      "Exception in thread Thread-5:\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/threading.py\", line 932, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 567, in run\n",
      "    self.flag_executor_shutting_down()\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 756, in flag_executor_shutting_down\n",
      "    self.kill_workers()\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\", line 766, in kill_workers\n",
      "    recursive_terminate(p)\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/backend/utils.py\", line 28, in recursive_terminate\n",
      "    _recursive_terminate_without_psutil(process)\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/backend/utils.py\", line 53, in _recursive_terminate_without_psutil\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    932\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 933\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    934\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[0;34m(future, timeout)\u001b[0m\n\u001b[1;32m    541\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 542\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    543\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/concurrent/futures/_base.py\u001b[0m in \u001b[0;36mresult\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    433\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 434\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    435\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    301\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 302\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1528\u001b[0m         \u001b[0;34m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1529\u001b[0;31m         evaluate_candidates(ParameterSampler(\n\u001b[0m\u001b[1;32m   1530\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_distributions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params)\u001b[0m\n\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 708\u001b[0;31m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[1;32m    709\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1053\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    954\u001b[0m                     \u001b[0mensure_ready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 955\u001b[0;31m                     \u001b[0mbackend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabort_everything\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_ready\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    956\u001b[0m                 \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mabort_everything\u001b[0;34m(self, ensure_ready)\u001b[0m\n\u001b[1;32m    560\u001b[0m         \"\"\"\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_workers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/executor.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self, kill_workers)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshutdown\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkill_workers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mkill_workers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/process_executor.py\u001b[0m in \u001b[0;36mshutdown\u001b[0;34m(self, wait, kill_workers)\u001b[0m\n\u001b[1;32m   1170\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mexecutor_manager_thread\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1171\u001b[0;31m             \u001b[0mexecutor_manager_thread\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1172\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/threading.py\u001b[0m in \u001b[0;36mjoin\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1010\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1011\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wait_for_tstate_lock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1012\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/threading.py\u001b[0m in \u001b[0;36m_wait_for_tstate_lock\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_is_stopped\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1027\u001b[0;31m         \u001b[0;32melif\u001b[0m \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1028\u001b[0m             \u001b[0mlock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-be63166ca80e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m                                \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mftwo_scorer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m                                verbose=True)\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mclf_rand_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclf_rand_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'\\n\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36minner_f\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     71\u001b[0m                           FutureWarning)\n\u001b[1;32m     72\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0minner_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    734\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 736\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    737\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    738\u001b[0m         \u001b[0;31m# For multi-metric evaluation, store the best_index_, best_params_ and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, exc_type, exc_value, traceback)\u001b[0m\n\u001b[1;32m    725\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    726\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__exit__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraceback\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 727\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_terminate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    728\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_managed_backend\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_terminate_backend\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    755\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_terminate_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    756\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 757\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    758\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    759\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mterminate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0;31m# calls, but cleanup the temporary resources that the Parallel call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;31m# created. This 'hack' requires a private, low-level operation.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m             self._workers._temp_folder_manager._unlink_temporary_resources(\n\u001b[0m\u001b[1;32m    552\u001b[0m                 \u001b[0mcontext_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparallel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m             )\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/_memmapping_reducer.py\u001b[0m in \u001b[0;36m_unlink_temporary_resources\u001b[0;34m(self, context_id)\u001b[0m\n\u001b[1;32m    625\u001b[0m                         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m                     )\n\u001b[0;32m--> 627\u001b[0;31m                 self._try_delete_folder(\n\u001b[0m\u001b[1;32m    628\u001b[0m                     \u001b[0mallow_non_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcontext_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    629\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/_memmapping_reducer.py\u001b[0m in \u001b[0;36m_try_delete_folder\u001b[0;34m(self, allow_non_empty, context_id)\u001b[0m\n\u001b[1;32m    652\u001b[0m             \u001b[0mtemp_folder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cached_temp_folders\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcontext_id\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    653\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 654\u001b[0;31m                 delete_folder(\n\u001b[0m\u001b[1;32m    655\u001b[0m                     \u001b[0mtemp_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_non_empty\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_non_empty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    656\u001b[0m                 )\n",
      "\u001b[0;32m~/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/disk.py\u001b[0m in \u001b[0;36mdelete_folder\u001b[0;34m(folder_path, onerror, allow_non_empty)\u001b[0m\n\u001b[1;32m    134\u001b[0m                         \u001b[0;31m# yet.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                         \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m                 \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mRM_SUBDIRS_RETRY_TIME\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    _recursive_terminate(process.pid)\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/site-packages/joblib/externals/loky/backend/utils.py\", line 92, in _recursive_terminate\n",
      "    children_pids = subprocess.check_output(\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/subprocess.py\", line 411, in check_output\n",
      "    return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n",
      "  File \"/Users/krisjohnson/anaconda3/envs/ml/lib/python3.8/subprocess.py\", line 512, in run\n",
      "    raise CalledProcessError(retcode, process.args,\n",
      "subprocess.CalledProcessError: Command '['pgrep', '-P', '1119']' died with <Signals.SIGINT: 2>.\n"
     ]
    }
   ],
   "source": [
    "best_score, best_est = 0, None\n",
    "best_ests_list, best_scores_list = [], []\n",
    "algos = [(log_pipe, log_params), (rf_pipe, rf_params)]\n",
    "\n",
    "for algo, params in algos:\n",
    "\n",
    "    clf_rand_cv = RandomizedSearchCV(estimator=algo, \n",
    "                               param_distributions=params,\n",
    "                               n_iter = 20,\n",
    "                               cv=3,\n",
    "                               n_jobs=-1,\n",
    "                               scoring=ftwo_scorer,\n",
    "                               verbose=True,\n",
    "                               random_state=random_seed)\n",
    "    clf_rand_cv.fit(X_train, y_train)\n",
    "    \n",
    "    print(clf_rand_cv.best_score_, end='\\n\\n')\n",
    "    if clf_rand_cv.best_score_ > best_score:\n",
    "        best_score, best_est = clf_rand_cv.best_score_, clf_rand_cv.best_estimator_\n",
    "    \n",
    "    best_scores_list.append(clf_rand_cv.best_score_)\n",
    "    best_ests_list.append(clf_rand_cv.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "manufactured-marathon",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'RandomizedSearchCV' object has no attribute 'best_estimator_'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-5f03841ef319>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclf_rand_cv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'RandomizedSearchCV' object has no attribute 'best_estimator_'"
     ]
    }
   ],
   "source": [
    "clf_rand_cv.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "subtle-circle",
   "metadata": {},
   "source": [
    "# Best Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "assisted-notice",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe = make_pipeline(\n",
    "                         imblearn.over_sampling.SMOTE(\n",
    "                             sampling_strategy=.9, # what new ratio of minority class to majority should be\n",
    "                             k_neighbors=3,  # number of neighbors to use to construct synthetic data\n",
    "                             n_jobs=-1       # use all available workers\n",
    "                         ),\n",
    "                         StandardScaler(),\n",
    "                         PCA(n_components=5),  # How many main PCA \"directions\" to use\n",
    "                         LogisticRegression(solver='saga',           # faster for large datasets\n",
    "                                           class_weight='balanced',  # Weights the log_loss function to account for class imablances\n",
    "                                           C=21.54,                  # 1/C is regularization strength, larger C means less regularization\n",
    "                                           n_jobs=-1))               # use all available workers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "particular-manhattan",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "optical-fantasy",
   "metadata": {},
   "source": [
    "# Best Overall Model - RandomForests\n",
    "\n",
    "I'm choosing the best model as the randomForest tuned for max depth of trees to be 25 and fitting 400 decision trees, even though it's recall, the metric we're most interested in, is slightly worse than the default. The precision is significantly higher, even though it's less than .5, meaning it'll significantly decrease the amount of noise. Too low of precision is just unuseable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "minimal-communications",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(imblearn.under_sampling.RandomUnderSampler(        # Because data is so large, undersampling non-ransomware labels\n",
    "                                              sampling_strategy=0.5),   # new ratio of minority class to majority, .5 is even split\n",
    "                        RandomForestClassifier(class_weight='balanced', # weights to help with imbalanced data calculate automatically\n",
    "                                               max_depth=25,            # max tree depth, deeper trees will overfit, shallow trees can't fully capture relationship\n",
    "                                               n_estimators=400,        # number of Trees to fit, more trees means more likely to hit the asymptotic limit of fitting\n",
    "                                               n_jobs=-1))              # use all workers available, speeds up training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-monthly",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = make_pipeline(\n",
    "    imblearn.under_sampling.RandomUnderSampler(        \n",
    "              sampling_strategy=0.5),   \n",
    "\n",
    "     RandomForestClassifier(class_weight='balanced', \n",
    "               max_depth=25,            \n",
    "               n_estimators=400,        \n",
    "               n_jobs=-1))             "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "vanilla-insulation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('randomundersampler',\n",
       "                 RandomUnderSampler(sampling_strategy=0.5)),\n",
       "                ('randomforestclassifier',\n",
       "                 RandomForestClassifier(class_weight='balanced', max_depth=25,\n",
       "                                        n_estimators=400, n_jobs=-1))])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "overhead-accommodation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'memory': None,\n",
       " 'steps': [('randomundersampler', RandomUnderSampler(sampling_strategy=0.5)),\n",
       "  ('randomforestclassifier',\n",
       "   RandomForestClassifier(class_weight='balanced', max_depth=25, n_estimators=400,\n",
       "                          n_jobs=-1))],\n",
       " 'verbose': False,\n",
       " 'randomundersampler': RandomUnderSampler(sampling_strategy=0.5),\n",
       " 'randomforestclassifier': RandomForestClassifier(class_weight='balanced', max_depth=25, n_estimators=400,\n",
       "                        n_jobs=-1),\n",
       " 'randomundersampler__random_state': None,\n",
       " 'randomundersampler__replacement': False,\n",
       " 'randomundersampler__sampling_strategy': 0.5,\n",
       " 'randomforestclassifier__bootstrap': True,\n",
       " 'randomforestclassifier__ccp_alpha': 0.0,\n",
       " 'randomforestclassifier__class_weight': 'balanced',\n",
       " 'randomforestclassifier__criterion': 'gini',\n",
       " 'randomforestclassifier__max_depth': 25,\n",
       " 'randomforestclassifier__max_features': 'auto',\n",
       " 'randomforestclassifier__max_leaf_nodes': None,\n",
       " 'randomforestclassifier__max_samples': None,\n",
       " 'randomforestclassifier__min_impurity_decrease': 0.0,\n",
       " 'randomforestclassifier__min_impurity_split': None,\n",
       " 'randomforestclassifier__min_samples_leaf': 1,\n",
       " 'randomforestclassifier__min_samples_split': 2,\n",
       " 'randomforestclassifier__min_weight_fraction_leaf': 0.0,\n",
       " 'randomforestclassifier__n_estimators': 400,\n",
       " 'randomforestclassifier__n_jobs': -1,\n",
       " 'randomforestclassifier__oob_score': False,\n",
       " 'randomforestclassifier__random_state': None,\n",
       " 'randomforestclassifier__verbose': 0,\n",
       " 'randomforestclassifier__warm_start': False}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "retired-cable",
   "metadata": {},
   "source": [
    "# Best Model Results on Test set\n",
    "\n",
    "So now that we have our best model we can see how it works on a the held out test set as a final evaluation. We'll use F1 score here since it's more interpretible as it's more commonly used. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-decision",
   "metadata": {},
   "outputs": [],
   "source": [
    "# using rf_pipe, definied right above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "several-pharmaceutical",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe.fit(X_train, y_train)  # retrain best model on all training data\n",
    "test_predictions = pipe.predict(X_test)  # create predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "alternative-banks",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Recall: 0.8204, Precision: 0.4281, and F1 score: 0.5626\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation metrics - primarily interested in Recall \n",
    "# need to know Precision as well, for context\n",
    "# and f1 score is helpful to describe their joint relationship and to a broader audience\n",
    "recall = recall_score(y_test, test_predictions)\n",
    "precision = precision_score(y_test, test_predictions)\n",
    "f1 = fbeta_score(y_test, test_predictions, beta=1)\n",
    "\n",
    "print('Best Recall: {:.4f}, Precision: {:.4f}, and F1 score: {:.4f}'.format(recall, precision, f1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sufficient-study",
   "metadata": {},
   "source": [
    "So our final Recall score is .82, meaning we'll identify actual ransomWare 82% of the time and miss it the other 18%. So about one in five actual ransomWare transactions will be missed and labeled as normal transactions.\n",
    "\n",
    "The downside is the Precision metric of .43 means that even though we'll catch 82% of ransomWare records, it's at the cost of our ransomWare guesses being wrong more than half the time. In other words, with a precision of .43, for every 100 times we predict ransomWare, we'll be right 43 times and wrong 57! That means a lot of wasted time looking at actual normal transaction records to try and verify if they're ransomWare or not.\n",
    "\n",
    "In the end, our results certainly aren't as robust as we'd like, with such a low precisiion we're going to be wading through a lot of false alarms if we put this into production to catch ransomWare in real time. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "copyrighted-magnet",
   "metadata": {},
   "source": [
    "# Next steps\n",
    "\n",
    "The initial data decription did point out that the 'white' labeled, or normal transactions, weren't all vetted to not be RansomWare. So for next steps we could turn this into a semi-supervised problem by taking the highest confidence False Positives, or predictions of ransomWare predictions that were labeled as normal, and dive into those further to see if they are in fact ransomWare. Then retrain our model on the updated data, look at the highest confidence False Positives to see if they're wrongly labeled, and do it all again. In this way we might uncover more ransomWare attacks that aren't being flagged, which would be a great win in further uncovering and identifying fraud in the blockchain. Exposing and stopping nefarious activity will ultimtely lead to more trust in the blockchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "novel-discrimination",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
